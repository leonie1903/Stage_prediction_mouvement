{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from nolds import dfa\n",
    "import warnings\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resampled = pd.read_csv('../data/Marion.lnk/MG_D.csv')\n",
    "\n",
    "column_names = df_resampled.columns\n",
    "df_resampled.columns = ['date_time', 'X', 'Y', 'Z']\n",
    "print(f\"New column names: {df_resampled.columns}\")\n",
    "\n",
    "# Convert 'date_time' to datetime format\n",
    "df_resampled['date_time'] = pd.to_datetime(df_resampled['date_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the activity data\n",
    "activity = pd.read_csv('../data/activity_M.csv', sep=';', skipinitialspace=True)\n",
    "\n",
    "# Remove spaces around column names\n",
    "activity.columns = activity.columns.str.strip()\n",
    "\n",
    "# Define the base date\n",
    "base_date = pd.to_datetime(\"2024-04-15\")\n",
    "\n",
    "# Add the base date to recorded days\n",
    "activity['date'] = base_date + pd.to_timedelta(activity['jour'], unit='D')\n",
    "\n",
    "# Function to clean and normalize time values\n",
    "def clean_time(time_str):\n",
    "    \"\"\"\n",
    "    Cleans and normalizes the time format.\n",
    "    - Removes spaces\n",
    "    - Converts 'h' to ':'\n",
    "    - Adds \":00\" if missing\n",
    "    - Ignores invalid values\n",
    "    \"\"\"\n",
    "    if pd.isna(time_str) or not isinstance(time_str, str) or time_str.strip() == \"\":\n",
    "        return np.nan  # Ignore empty values\n",
    "    \n",
    "    time_str = time_str.strip().replace(\"h\", \":\")  # Convert format '9h30' -> '9:30'\n",
    "    \n",
    "    if \":\" not in time_str:\n",
    "        return time_str + \":00\"  # Add seconds if missing\n",
    "    elif time_str.count(\":\") == 1:\n",
    "        return time_str + \":00\"  # Ensure HH:MM:SS format\n",
    "    \n",
    "    return time_str  # Return correctly formatted time\n",
    "\n",
    "# Apply the correction to the 'start' and 'end' columns\n",
    "activity['debut'] = activity['debut'].astype(str).apply(clean_time)\n",
    "activity['fin'] = activity['fin'].astype(str).apply(clean_time)\n",
    "\n",
    "# Merge date and time to obtain a proper datetime format\n",
    "activity['debut'] = pd.to_datetime(activity['date'].astype(str) + \" \" + activity['debut'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "activity['fin'] = pd.to_datetime(activity['date'].astype(str) + \" \" + activity['fin'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "# Remove the temporary 'date' column if it is no longer needed\n",
    "activity.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# Display the first rows of the updated DataFrame\n",
    "print(activity.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InactivityDetector:\n",
    "    def __init__(self, df, activity, epoch_size=1, inactivity_threshold='dynamic', delta_threshold=0.01, gravity_threshold=0.9):\n",
    "        \"\"\"\n",
    "        Initializes the InactivityDetector.\n",
    "        \n",
    "        Parameters:\n",
    "        - df: DataFrame containing accelerometer data (columns: ['date_time', 'X', 'Y', 'Z']).\n",
    "        - activity: DataFrame containing activity logs with start and end times.\n",
    "        - epoch_size: Time window (in seconds) for aggregation.\n",
    "        - inactivity_threshold: Acceleration threshold for inactivity detection ('dynamic' or fixed value).\n",
    "        - delta_threshold: Threshold for acceleration variation.\n",
    "        - gravity_threshold: Threshold to consider values close to Earth's gravity.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.activity = activity\n",
    "        self.epoch_size = epoch_size\n",
    "        self.inactivity_threshold = inactivity_threshold\n",
    "        self.delta_threshold = delta_threshold\n",
    "        self.gravity_threshold = gravity_threshold\n",
    "        self.processed_data = None\n",
    "        self._prepare_data()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Computes overall acceleration and prepares the data.\"\"\"\n",
    "        self.df['acceleration'] = np.sqrt(self.df['X']**2 + self.df['Y']**2 + self.df['Z']**2)\n",
    "        self.df['timestamp'] = pd.to_datetime(self.df['date_time'])\n",
    "        self.df = self.df.sort_values('timestamp')\n",
    "        self.df['epoch'] = self.df['timestamp'].dt.floor(f'{self.epoch_size}s')\n",
    "        self.df['date'] = self.df['timestamp'].dt.date\n",
    "        self.df['delta_acc'] = self.df['acceleration'].diff().abs()\n",
    "        \n",
    "        self.activity['debut'] = pd.to_datetime(self.activity['debut'])\n",
    "        self.activity['fin'] = pd.to_datetime(self.activity['fin'])\n",
    "\n",
    "        def detect_inactivity(self):\n",
    "        \"\"\"Detects inactivity periods using an acceleration threshold and acceleration variation threshold.\"\"\"\n",
    "        grouped = self.df.groupby(['epoch', 'date']).agg({'acceleration': 'mean', 'delta_acc': 'mean'}).reset_index()\n",
    "        \n",
    "        if self.inactivity_threshold == 'dynamic':\n",
    "            mean_acc = grouped['acceleration'].mean()\n",
    "            std_acc = grouped['acceleration'].std()\n",
    "            threshold = max(0.1, mean_acc - 0.5 * std_acc)\n",
    "        else:\n",
    "            threshold = self.inactivity_threshold\n",
    "        \n",
    "        grouped['inactive'] = ((grouped['acceleration'] < threshold) | \n",
    "                               ((grouped['delta_acc'] < self.delta_threshold) & \n",
    "                                (np.abs(grouped['acceleration'] - 1) < self.gravity_threshold)))\n",
    "        grouped['inactive_group'] = (grouped['inactive'] != grouped['inactive'].shift()).cumsum()\n",
    "        \n",
    "        self.processed_data = grouped\n",
    "        return grouped\n",
    "    \n",
    "    def visualize_inactivity_per_day(self):\n",
    "        \"\"\"Displays inactivity periods and activities with better visibility.\"\"\"\n",
    "        if self.processed_data is None:\n",
    "            self.detect_inactivity()\n",
    "\n",
    "        color_map = px.colors.qualitative.Set1\n",
    "        activity_types = self.activity['activite'].unique()\n",
    "        activity_colors = {activity: color_map[i % len(color_map)] for i, activity in enumerate(activity_types)}\n",
    "\n",
    "        for day in self.processed_data['date'].unique():\n",
    "            daily_data = self.processed_data[self.processed_data['date'] == day]\n",
    "            fig = go.Figure()\n",
    "\n",
    "            # Acceleration curve\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=daily_data['epoch'], y=daily_data['acceleration'], \n",
    "                mode='lines', name='Acceleration', line=dict(color='blue', width=2)\n",
    "            ))\n",
    "\n",
    "            # Inactivity periods in transparency\n",
    "            inactive_added = False  # Pour éviter de répéter la légende\n",
    "            for _, group in daily_data.groupby('inactive_group'):\n",
    "                if group['inactive'].iloc[0]:\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=group['epoch'], y=group['acceleration'],\n",
    "                        mode='lines', fill='tozeroy', \n",
    "                        fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "                        line=dict(width=0), \n",
    "                        name='Inactive Periods' if not inactive_added else None,\n",
    "                        showlegend=not inactive_added  # Affiche la légende une seule fois\n",
    "                    ))\n",
    "                    inactive_added = True\n",
    "\n",
    "            # Adding activities in the background as colored zones\n",
    "            for _, row in self.activity[self.activity['debut'].dt.date == day].iterrows():\n",
    "                activity_color = activity_colors.get(row['activite'], 'rgba(0, 100, 255, 0.3)')\n",
    "                fig.add_shape(\n",
    "                    type=\"rect\", x0=row['debut'], x1=row['fin'], \n",
    "                    y0=daily_data['acceleration'].min(), y1=daily_data['acceleration'].max(), \n",
    "                    fillcolor=activity_color, line=dict(width=0), opacity=0.3\n",
    "                )\n",
    "                fig.add_annotation(\n",
    "                    x=row['debut'], y=daily_data['acceleration'].max(),\n",
    "                    text=row['activite'], showarrow=False,\n",
    "                    font=dict(size=10, color=\"black\"),\n",
    "                    bgcolor=\"rgba(255,255,255,0.7)\"\n",
    "                )\n",
    "\n",
    "            fig.update_layout(\n",
    "                title=f\"Inactivity Periods & Activities - {day}\",\n",
    "                xaxis_title=\"Time\", yaxis_title=\"Acceleration\",\n",
    "                hovermode=\"x\", template=\"plotly_white\"\n",
    "            )\n",
    "            fig.show()\n",
    "\n",
    "\n",
    "# Run the detector\n",
    "detector = InactivityDetector(df_resampled, activity, epoch_size=1, inactivity_threshold='dynamic', delta_threshold=0.01, gravity_threshold=0.9)\n",
    "detector.detect_inactivity()\n",
    "detector.visualize_inactivity_per_day()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "class DFAAnalysis:\n",
    "    def __init__(self, df, activity, inactivity_data, window_size=300, step_size=50):\n",
    "        \"\"\"\n",
    "        Initializes the DFAAnalysis class.\n",
    "        \n",
    "        Parameters:\n",
    "        - df: DataFrame containing time-series accelerometer data.\n",
    "        - activity: DataFrame with activity logs (excluding inactive periods).\n",
    "        - inactivity_data: DataFrame indicating inactivity periods.\n",
    "        - window_size: Size of the moving window for DFA calculation.\n",
    "        - step_size: Step size for sliding window analysis.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.activity = activity[activity['activite'] != \"Inactif\"]  # Exclude inactivity periods\n",
    "        self.inactivity_data = inactivity_data\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        self.dfa_results = {}\n",
    "        self.dfa_min = None\n",
    "        self.dfa_max = None\n",
    "    \n",
    "    def filter_active_periods(self):\n",
    "        \"\"\"Filters out inactive periods from the dataset.\"\"\"\n",
    "        self.df['epoch'] = pd.to_datetime(self.df['epoch'])\n",
    "        self.inactivity_data['epoch'] = pd.to_datetime(self.inactivity_data['epoch'])\n",
    "        \n",
    "        active_df = self.df.merge(self.inactivity_data[['epoch', 'inactive']], on='epoch', how='left')\n",
    "\n",
    "        # Ensure correct column names after merging\n",
    "        if 'inactive_y' in active_df.columns:\n",
    "            active_df.rename(columns={'inactive_y': 'inactive'}, inplace=True)\n",
    "        if 'inactive_x' in active_df.columns:\n",
    "            active_df.drop(columns=['inactive_x'], inplace=True)\n",
    "\n",
    "        active_df['inactive'].fillna(False, inplace=True)\n",
    "        return active_df[~active_df['inactive']].drop(columns=['inactive'])  # Keep only active periods\n",
    "    \n",
    "    def compute_dfa(self):\n",
    "        \"\"\"Computes the DFA exponent over active periods, excluding inactivity.\"\"\"\n",
    "        self.df['date'] = self.df['timestamp'].dt.date  \n",
    "        unique_dates = self.df['date'].unique()\n",
    "        dfa_all_values = []\n",
    "\n",
    "        for day in unique_dates:\n",
    "            active_df = self.filter_active_periods()\n",
    "            active_daily_df = active_df[active_df['date'] == day]\n",
    "\n",
    "            series = active_daily_df['acceleration'].values\n",
    "            dfa_values = []\n",
    "            time_stamps = []\n",
    "\n",
    "            for i in range(0, len(series) - self.window_size, self.step_size):\n",
    "                window = series[i:i + self.window_size]\n",
    "                if len(window) == self.window_size:\n",
    "                    alpha = dfa(window)\n",
    "                    dfa_values.append(alpha)\n",
    "                    time_stamps.append(active_daily_df['timestamp'].iloc[i])\n",
    "                    dfa_all_values.append(alpha)\n",
    "\n",
    "            self.dfa_results[day] = {'timestamps': time_stamps, 'dfa_values': dfa_values}\n",
    "\n",
    "        # Set a common scale for all plots\n",
    "        if dfa_all_values:\n",
    "            self.dfa_min = min(dfa_all_values)\n",
    "            self.dfa_max = max(dfa_all_values)\n",
    "    \n",
    "    def visualize_dfa_plotly(self):\n",
    "        \"\"\"Displays DFA exponent over time with activity background using Plotly.\"\"\"\n",
    "        color_map = sns.color_palette(\"husl\", n_colors=len(self.activity['activite'].unique()))\n",
    "        activity_colors = {act: f'rgba({int(r*255)}, {int(g*255)}, {int(b*255)}, 0.5)' \n",
    "                           for act, (r, g, b) in zip(self.activity['activite'].unique(), color_map)}\n",
    "        \n",
    "        for day, results in self.dfa_results.items():\n",
    "            fig = go.Figure()\n",
    "\n",
    "            # Plot DFA exponent\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=results['timestamps'], y=results['dfa_values'],\n",
    "                mode='lines+markers', name='DFA Exponent'\n",
    "            ))\n",
    "\n",
    "            # Add background activities\n",
    "            daily_activities = self.activity[self.activity['debut'].dt.date == day]\n",
    "            for _, row in daily_activities.iterrows():\n",
    "                fig.add_shape(\n",
    "                    type='rect', x0=row['debut'], x1=row['fin'], \n",
    "                    y0=self.dfa_min, y1=self.dfa_max,  # Fixed scale\n",
    "                    fillcolor=activity_colors.get(row['activite'], 'rgba(100,100,100,0.3)'),\n",
    "                    line=dict(width=0), opacity=0.5\n",
    "                )\n",
    "                fig.add_annotation(\n",
    "                    x=row['debut'], y=self.dfa_max,\n",
    "                    text=row['activite'], showarrow=False, font=dict(size=10, color='black'),\n",
    "                    bgcolor='rgba(255,255,255,0.7)'\n",
    "                )\n",
    "\n",
    "            fig.update_layout(\n",
    "                title=f'DFA Exponent Over Time - {day}',\n",
    "                xaxis_title='Time', yaxis_title='DFA Exponent',\n",
    "                hovermode='x unified', template='plotly_white',\n",
    "                yaxis=dict(range=[self.dfa_min, self.dfa_max])  # Apply common scale\n",
    "            )\n",
    "            fig.show()\n",
    "\n",
    "# Usage\n",
    "detector = InactivityDetector(df_resampled, activity)\n",
    "detector.detect_inactivity()\n",
    "inactivity_data = detector.processed_data\n",
    "\n",
    "detector.processed_data['timestamp'] = detector.processed_data['epoch']\n",
    "dfa_analysis = DFAAnalysis(detector.processed_data, activity, inactivity_data, window_size=300, step_size=50)\n",
    "dfa_analysis.compute_dfa()\n",
    "dfa_analysis.visualize_dfa_plotly()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# ÉTAPE 1 : Calcul du DFA exponentiel\n",
    "class DFAAnalysis:\n",
    "    def __init__(self, df, activity, inactivity_data, window_size=300, step_size=50):\n",
    "        \"\"\"\n",
    "        Initializes the DFAAnalysis class.\n",
    "        \n",
    "        Parameters:\n",
    "        - df: DataFrame containing time-series accelerometer data.\n",
    "        - activity: DataFrame with activity logs (excluding inactive periods).\n",
    "        - inactivity_data: DataFrame indicating inactivity periods.\n",
    "        - window_size: Size of the moving window for DFA calculation.\n",
    "        - step_size: Step size for sliding window analysis.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.activity = activity[activity['activite'] != \"Inactif\"]  # Exclude inactivity periods\n",
    "        self.inactivity_data = inactivity_data\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        self.dfa_results = {}\n",
    "        self.dfa_min = None\n",
    "        self.dfa_max = None\n",
    "    \n",
    "    def filter_active_periods(self):\n",
    "        \"\"\"Filters out inactive periods from the dataset.\"\"\"\n",
    "        self.df['epoch'] = pd.to_datetime(self.df['epoch'])\n",
    "        self.inactivity_data['epoch'] = pd.to_datetime(self.inactivity_data['epoch'])\n",
    "        \n",
    "        active_df = self.df.merge(self.inactivity_data[['epoch', 'inactive']], on='epoch', how='left')\n",
    "\n",
    "        # Ensure correct column names after merging\n",
    "        if 'inactive_y' in active_df.columns:\n",
    "            active_df.rename(columns={'inactive_y': 'inactive'}, inplace=True)\n",
    "        if 'inactive_x' in active_df.columns:\n",
    "            active_df.drop(columns=['inactive_x'], inplace=True)\n",
    "\n",
    "        active_df['inactive'].fillna(False, inplace=True)\n",
    "        return active_df[~active_df['inactive']].drop(columns=['inactive'])  # Keep only active periods\n",
    "    \n",
    "    def compute_dfa(self):\n",
    "        \"\"\"Computes the DFA exponent over active periods, excluding inactivity.\"\"\"\n",
    "        self.df['date'] = self.df['timestamp'].dt.date  \n",
    "        unique_dates = self.df['date'].unique()\n",
    "        dfa_all_values = []\n",
    "\n",
    "        for day in unique_dates:\n",
    "            active_df = self.filter_active_periods()\n",
    "            active_daily_df = active_df[active_df['date'] == day]\n",
    "\n",
    "            series = active_daily_df['acceleration'].values\n",
    "            dfa_values = []\n",
    "            time_stamps = []\n",
    "\n",
    "            for i in range(0, len(series) - self.window_size, self.step_size):\n",
    "                window = series[i:i + self.window_size]\n",
    "                if len(window) == self.window_size:\n",
    "                    alpha = dfa(window)  # DFA function must be defined elsewhere\n",
    "                    dfa_values.append(alpha)\n",
    "                    time_stamps.append(active_daily_df['timestamp'].iloc[i])\n",
    "                    dfa_all_values.append(alpha)\n",
    "\n",
    "            self.dfa_results[day] = {'timestamps': time_stamps, 'dfa_values': dfa_values}\n",
    "\n",
    "        # Set a common scale for all plots\n",
    "        if dfa_all_values:\n",
    "            self.dfa_min = min(dfa_all_values)\n",
    "            self.dfa_max = max(dfa_all_values)\n",
    "    \n",
    "    def visualize_dfa_plotly(self):\n",
    "        \"\"\"Displays DFA exponent over time with activity background using Plotly.\"\"\"\n",
    "        color_map = sns.color_palette(\"husl\", n_colors=len(self.activity['activite'].unique()))\n",
    "        activity_colors = {act: f'rgba({int(r*255)}, {int(g*255)}, {int(b*255)}, 0.5)' \n",
    "                           for act, (r, g, b) in zip(self.activity['activite'].unique(), color_map)}\n",
    "        \n",
    "        for day, results in self.dfa_results.items():\n",
    "            fig = go.Figure()\n",
    "\n",
    "            # Plot DFA exponent\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=results['timestamps'], y=results['dfa_values'],\n",
    "                mode='lines+markers', name='DFA Exponent'\n",
    "            ))\n",
    "\n",
    "            # Add background activities\n",
    "            daily_activities = self.activity[self.activity['debut'].dt.date == day]\n",
    "            for _, row in daily_activities.iterrows():\n",
    "                fig.add_shape(\n",
    "                    type='rect', x0=row['debut'], x1=row['fin'], \n",
    "                    y0=self.dfa_min, y1=self.dfa_max,  \n",
    "                    fillcolor=activity_colors.get(row['activite'], 'rgba(100,100,100,0.3)'),\n",
    "                    line=dict(width=0), opacity=0.5\n",
    "                )\n",
    "                fig.add_annotation(\n",
    "                    x=row['debut'], y=self.dfa_max,\n",
    "                    text=row['activite'], showarrow=False, font=dict(size=10, color='black'),\n",
    "                    bgcolor='rgba(255,255,255,0.7)'\n",
    "                )\n",
    "\n",
    "            fig.update_layout(\n",
    "                title=f'DFA Exponent Over Time - {day}',\n",
    "                xaxis_title='Time', yaxis_title='DFA Exponent',\n",
    "                hovermode='x unified', template='plotly_white',\n",
    "                yaxis=dict(range=[self.dfa_min, self.dfa_max])\n",
    "            )\n",
    "            fig.show()\n",
    "\n",
    "\n",
    "# ÉTAPE 2 : Comparaison des DFA exponentiels entre occurrences d’une même activité\n",
    "\n",
    "def compute_activity_correlation(dfa_analysis):\n",
    "    \"\"\"\n",
    "    Calcule la corrélation de Pearson entre les DFA exponentiels des occurrences \n",
    "    d'une même activité et génère une heatmap pour visualiser la similarité.\n",
    "    \"\"\"\n",
    "    activity_dfa = {}\n",
    "\n",
    "    # Récupérer les DFA exponentiels pour chaque occurrence d'une activité\n",
    "    for day, results in dfa_analysis.dfa_results.items():\n",
    "        timestamps = np.array(results['timestamps'])  # Conversion en tableau NumPy\n",
    "        dfa_values = np.array(results['dfa_values'])  # Conversion en tableau NumPy\n",
    "        \n",
    "        for _, row in dfa_analysis.activity[dfa_analysis.activity['debut'].dt.date == day].iterrows():\n",
    "            activity = row['activite']\n",
    "            mask = (timestamps >= row['debut']) & (timestamps <= row['fin'])  # Masque corrigé\n",
    "            \n",
    "            if np.any(mask):\n",
    "                if activity not in activity_dfa:\n",
    "                    activity_dfa[activity] = []\n",
    "                activity_dfa[activity].append(dfa_values[mask])  # Stocker les valeurs filtrées\n",
    "\n",
    "    # Générer une heatmap pour chaque activité ayant au moins 2 occurrences\n",
    "    for activity, dfa_series in activity_dfa.items():\n",
    "        if len(dfa_series) < 2:\n",
    "            continue  \n",
    "\n",
    "        # Normalisation optionnelle (Z-score)\n",
    "        dfa_series = [((series - np.mean(series)) / np.std(series)) if np.std(series) != 0 else series for series in dfa_series]\n",
    "\n",
    "        # Construire la matrice de corrélation\n",
    "        dfa_matrix = np.array([np.interp(np.linspace(0, 1, 100), np.linspace(0, 1, len(series)), series) for series in dfa_series])\n",
    "        corr_matrix = np.corrcoef(dfa_matrix)\n",
    "\n",
    "        # Afficher la heatmap\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "        plt.title(f\"Corrélation DFA - Activité: {activity}\")\n",
    "        plt.xlabel(\"Occurrence\")\n",
    "        plt.ylabel(\"Occurrence\")\n",
    "        plt.show()\n",
    "\n",
    "detector = InactivityDetector(df_resampled, activity)\n",
    "detector.detect_inactivity()\n",
    "inactivity_data = detector.processed_data\n",
    "\n",
    "detector.processed_data['timestamp'] = detector.processed_data['epoch']\n",
    "dfa_analysis = DFAAnalysis(detector.processed_data, activity, inactivity_data, window_size=300, step_size=50)\n",
    "dfa_analysis.compute_dfa()\n",
    "dfa_analysis.visualize_dfa_plotly()\n",
    "\n",
    "#  heatmap \n",
    "compute_activity_correlation(dfa_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def compute_activity_distance(dfa_analysis):\n",
    "    \"\"\"\n",
    "    Calcule la distance euclidienne entre les DFA exponentiels des occurrences \n",
    "    d'une même activité et génère une heatmap pour visualiser les différences.\n",
    "    \"\"\"\n",
    "    activity_dfa = {}\n",
    "\n",
    "    # Récupérer les DFA exponentiels pour chaque occurrence d'une activité\n",
    "    for day, results in dfa_analysis.dfa_results.items():\n",
    "        timestamps = np.array(results['timestamps'])  # Conversion en tableau NumPy\n",
    "        dfa_values = np.array(results['dfa_values'])  # Conversion en tableau NumPy\n",
    "        \n",
    "        for _, row in dfa_analysis.activity[dfa_analysis.activity['debut'].dt.date == day].iterrows():\n",
    "            activity = row['activite']\n",
    "            mask = (timestamps >= row['debut']) & (timestamps <= row['fin'])  # Masque pour filtrer\n",
    "            \n",
    "            if np.any(mask):\n",
    "                if activity not in activity_dfa:\n",
    "                    activity_dfa[activity] = []\n",
    "                activity_dfa[activity].append(dfa_values[mask])  # Stocker les valeurs filtrées\n",
    "\n",
    "    # Générer une heatmap pour chaque activité ayant au moins 2 occurrences\n",
    "    for activity, dfa_series in activity_dfa.items():\n",
    "        if len(dfa_series) < 2:\n",
    "            continue  # Passer si l'activité n'a pas assez d'occurrences\n",
    "\n",
    "        # Interpolation pour aligner les longueurs\n",
    "        dfa_matrix = np.array([np.interp(np.linspace(0, 1, 100), np.linspace(0, 1, len(series)), series) for series in dfa_series])\n",
    "\n",
    "        # Calculer la distance euclidienne entre les occurrences\n",
    "        dist_matrix = cdist(dfa_matrix, dfa_matrix, metric='euclidean')\n",
    "\n",
    "        # Afficher la heatmap\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(dist_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "        plt.title(f\"Distance Euclidienne DFA - Activité: {activity}\")\n",
    "        plt.xlabel(\"Occurrence\")\n",
    "        plt.ylabel(\"Occurrence\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Lancement du programme avec Distance Euclidienne\n",
    "\n",
    "compute_activity_distance(dfa_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtw import accelerated_dtw\n",
    "\n",
    "def compute_activity_dtw(dfa_analysis):\n",
    "    \"\"\"\n",
    "    Calcule la distance DTW entre les DFA exponentiels des occurrences \n",
    "    d'une même activité et génère une heatmap pour visualiser la similarité.\n",
    "    \"\"\"\n",
    "    activity_dfa = {}\n",
    "\n",
    "    # Récupérer les DFA exponentiels pour chaque occurrence d'une activité\n",
    "    for day, results in dfa_analysis.dfa_results.items():\n",
    "        timestamps = np.array(results['timestamps'])  # Conversion en tableau NumPy\n",
    "        dfa_values = np.array(results['dfa_values'])  # Conversion en tableau NumPy\n",
    "        \n",
    "        for _, row in dfa_analysis.activity[dfa_analysis.activity['debut'].dt.date == day].iterrows():\n",
    "            activity = row['activite']\n",
    "            mask = (timestamps >= row['debut']) & (timestamps <= row['fin'])  # Masque pour filtrer\n",
    "            \n",
    "            if np.any(mask):\n",
    "                if activity not in activity_dfa:\n",
    "                    activity_dfa[activity] = []\n",
    "                activity_dfa[activity].append(dfa_values[mask])  # Stocker les valeurs filtrées\n",
    "\n",
    "    # Générer une heatmap pour chaque activité ayant au moins 2 occurrences\n",
    "    for activity, dfa_series in activity_dfa.items():\n",
    "        if len(dfa_series) < 2:\n",
    "            continue  # Passer si l'activité n'a pas assez d'occurrences\n",
    "\n",
    "        # Calcul de la matrice DTW\n",
    "        n = len(dfa_series)\n",
    "        dtw_matrix = np.zeros((n, n))\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    dtw_matrix[i, j], _, _, _ = accelerated_dtw(dfa_series[i], dfa_series[j], dist='euclidean')\n",
    "\n",
    "        # Afficher la heatmap\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(dtw_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "        plt.title(f\"Distance DTW DFA - Activité: {activity}\")\n",
    "        plt.xlabel(\"Occurrence\")\n",
    "        plt.ylabel(\"Occurrence\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Lancement du programme avec DTW\n",
    "\n",
    "compute_activity_dtw(dfa_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "\n",
    "def create_activity_motifs(dfa_analysis):\n",
    "    \"\"\"\n",
    "    Génère un motif unique par activité en agrégeant les données DFA de plusieurs jours.\n",
    "    \"\"\"\n",
    "    activity_motifs = {}\n",
    "\n",
    "    # Récupération et regroupement des DFA par activité\n",
    "    activity_dfa = {}\n",
    "    for day, results in dfa_analysis.dfa_results.items():\n",
    "        timestamps = np.array(results['timestamps'])  # Timestamps\n",
    "        dfa_values = np.array(results['dfa_values'])  # Valeurs DFA\n",
    "        \n",
    "        # Associer chaque activité à ses segments DFA\n",
    "        for _, row in dfa_analysis.activity[dfa_analysis.activity['debut'].dt.date == day].iterrows():\n",
    "            activity = row['activite']\n",
    "            mask = (timestamps >= row['debut']) & (timestamps <= row['fin'])\n",
    "            if np.any(mask):\n",
    "                if activity not in activity_dfa:\n",
    "                    activity_dfa[activity] = []\n",
    "                activity_dfa[activity].append(dfa_values[mask])  # Stockage des segments DFA\n",
    "\n",
    "    # Calcul des motifs types par activité\n",
    "    for activity, dfa_series in activity_dfa.items():\n",
    "        if len(dfa_series) < 2:\n",
    "            print(f\"⚠️ Pas assez d'occurrences pour {activity}, motif non créé.\")\n",
    "            continue\n",
    "\n",
    "        # Conversion en format compatible avec tslearn\n",
    "        dfa_series = [np.array(series, dtype=float) for series in dfa_series]\n",
    "        dfa_series = to_time_series_dataset(dfa_series)\n",
    "\n",
    "        # Calcul du barycentre DTW\n",
    "        motif = dtw_barycenter_averaging(dfa_series)\n",
    "\n",
    "        activity_motifs[activity] = motif\n",
    "\n",
    "    return activity_motifs\n",
    "\n",
    "# Création des motifs types\n",
    "activity_motifs = create_activity_motifs(dfa_analysis)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\" Motifs par activité générés :\", activity_motifs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activity_motifs(activity_motifs):\n",
    "    \"\"\"\n",
    "    Affiche les motifs types pour chaque activité.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    for activity, motif in activity_motifs.items():\n",
    "        plt.plot(motif.ravel(), label=f\"Activité: {activity}\")\n",
    "\n",
    "    plt.title(\"Motifs types DFA par activité\")\n",
    "    plt.xlabel(\"Temps\")\n",
    "    plt.ylabel(\"Valeur DFA\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_activity_motifs(activity_motifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../data/Marion.lnk/................csv')\n",
    "\n",
    "column_names = df1.columns\n",
    "df1.columns = ['date_time', 'X', 'Y', 'Z']\n",
    "print(f\"New column names: {df1.columns}\")\n",
    "\n",
    "# Convert 'date_time' to datetime format\n",
    "df1['date_time'] = pd.to_datetime(df1['date_time'])\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InactivityDetector:\n",
    "    def __init__(self, df, activity, epoch_size=1, inactivity_threshold='dynamic', delta_threshold=0.01, gravity_threshold=0.9):\n",
    "        \"\"\"\n",
    "        Détecte les périodes d'inactivité à partir des données d'accéléromètre.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.activity = activity\n",
    "        self.epoch_size = epoch_size\n",
    "        self.inactivity_threshold = inactivity_threshold\n",
    "        self.delta_threshold = delta_threshold\n",
    "        self.gravity_threshold = gravity_threshold\n",
    "        self.processed_data = None\n",
    "        self._prepare_data()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Prépare les données et calcule l'accélération globale.\"\"\"\n",
    "        self.df['acceleration'] = np.sqrt(self.df['X']**2 + self.df['Y']**2 + self.df['Z']**2)\n",
    "        self.df['timestamp'] = pd.to_datetime(self.df['date_time'])\n",
    "        self.df = self.df.sort_values('timestamp')\n",
    "        self.df['epoch'] = self.df['timestamp'].dt.floor(f'{self.epoch_size}s')\n",
    "        self.df['date'] = self.df['timestamp'].dt.date\n",
    "        self.df['delta_acc'] = self.df['acceleration'].diff().abs()\n",
    "        \n",
    "        self.activity['debut'] = pd.to_datetime(self.activity['debut'])\n",
    "        self.activity['fin'] = pd.to_datetime(self.activity['fin'])\n",
    "    \n",
    "    def detect_inactivity(self):\n",
    "        \"\"\"Détecte les périodes d'inactivité.\"\"\"\n",
    "        grouped = self.df.groupby(['epoch', 'date']).agg({'acceleration': 'mean', 'delta_acc': 'mean'}).reset_index()\n",
    "        \n",
    "        if self.inactivity_threshold == 'dynamic':\n",
    "            mean_acc = grouped['acceleration'].mean()\n",
    "            std_acc = grouped['acceleration'].std()\n",
    "            threshold = max(0.1, mean_acc - 0.5 * std_acc)\n",
    "        else:\n",
    "            threshold = self.inactivity_threshold\n",
    "        \n",
    "        grouped['inactive'] = ((grouped['acceleration'] < threshold) | \n",
    "                               ((grouped['delta_acc'] < self.delta_threshold) & \n",
    "                                (np.abs(grouped['acceleration'] - 1) < self.gravity_threshold)))\n",
    "        grouped['inactive_group'] = (grouped['inactive'] != grouped['inactive'].shift()).cumsum()\n",
    "        \n",
    "        self.processed_data = grouped\n",
    "        return grouped\n",
    "    \n",
    "# Détection de l’inactivité\n",
    "detector = InactivityDetector(df1, activity)\n",
    "detector.detect_inactivity()\n",
    "df1_active = detector.df.merge(detector.processed_data[['epoch', 'inactive']], on='epoch', how='left')\n",
    "df1_active = df1_active[~df1_active['inactive']].copy()\n",
    "print(\"Inactivité détectée et supprimée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "\n",
    "class DFAAnalysis:\n",
    "    def __init__(self, df, window_size=300, step_size=50):\n",
    "        \"\"\"\n",
    "        Analyse DFA sur les périodes actives uniquement.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        self.dfa_results = {}\n",
    "\n",
    "    def compute_dfa(self):\n",
    "        \"\"\"Calcule l’exposant DFA sur les périodes actives du DataFrame.\"\"\"\n",
    "        self.df['date'] = self.df['date_time'].dt.date  \n",
    "        unique_dates = self.df['date'].unique()\n",
    "\n",
    "        for day in unique_dates:\n",
    "            daily_df = self.df[self.df['date'] == day]\n",
    "            series = daily_df['acceleration'].values\n",
    "            dfa_values = []\n",
    "            time_stamps = []\n",
    "\n",
    "            for i in range(0, len(series) - self.window_size, self.step_size):\n",
    "                window = series[i:i + self.window_size]\n",
    "                if len(window) == self.window_size:\n",
    "                    alpha = np.std(window)  # Simplification temporaire du calcul DFA\n",
    "                    dfa_values.append(alpha)\n",
    "                    time_stamps.append(daily_df['date_time'].iloc[i])\n",
    "\n",
    "            self.dfa_results[day] = {'timestamps': time_stamps, 'dfa_values': dfa_values}\n",
    "\n",
    "# Calcul du DFA\n",
    "dfa_analysis = DFAAnalysis(df1_active)\n",
    "dfa_analysis.compute_dfa()\n",
    "print(\"DFA calculé sur les périodes actives.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tslearn.metrics import dtw\n",
    "from tslearn.utils import to_time_series\n",
    "\n",
    "def compute_similarity_score(segment, motif):\n",
    "    \"\"\"\n",
    "    Calcule la similarité entre un segment DFA et un motif type en utilisant DTW.\n",
    "    \"\"\"\n",
    "    distance = dtw(segment, motif)\n",
    "    max_distance = max(np.linalg.norm(segment), np.linalg.norm(motif))\n",
    "    similarity = 1 - (distance / max_distance) if max_distance > 0 else 0\n",
    "    return similarity\n",
    "\n",
    "def recognize_activities(dfa_analysis, activity_motifs, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Identifie les activités sur le second jeu de données en comparant\n",
    "    chaque segment DFA aux motifs types avec un seuil de similarité.\n",
    "    \"\"\"\n",
    "    recognized_activities = []\n",
    "    \n",
    "    for day, results in dfa_analysis.dfa_results.items():\n",
    "        timestamps = np.array(results['timestamps'])\n",
    "        dfa_values = np.array(results['dfa_values'])\n",
    "        \n",
    "        for i, segment in enumerate(dfa_values):\n",
    "            best_match = None\n",
    "            best_score = 0\n",
    "            \n",
    "            for activity, motif in activity_motifs.items():\n",
    "                similarity = compute_similarity_score(segment, motif)\n",
    "                if similarity > best_score and similarity >= similarity_threshold:\n",
    "                    best_score = similarity\n",
    "                    best_match = activity\n",
    "            \n",
    "            if best_match:\n",
    "                recognized_activities.append({\n",
    "                    'timestamp': timestamps[i],\n",
    "                    'activity': best_match,\n",
    "                    'similarity': best_score\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(recognized_activities)\n",
    "\n",
    "# Reconnaissance des activités\n",
    "recognized_activities_df = recognize_activities(dfa_analysis, activity_motifs, similarity_threshold=0.8)\n",
    "\n",
    "# Affichage des résultats\n",
    "def display_recognized_activities(recognized_activities_df):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for activity in recognized_activities_df['activity'].unique():\n",
    "        subset = recognized_activities_df[recognized_activities_df['activity'] == activity]\n",
    "        plt.scatter(subset['timestamp'], [activity] * len(subset), label=activity)\n",
    "    \n",
    "    plt.xlabel(\"Temps\")\n",
    "    plt.ylabel(\"Activité reconnue\")\n",
    "    plt.title(\"Activités reconnues sur le second jeu de données\")\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "display_recognized_activities(recognized_activities_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_score(segment, motif):\n",
    "    \"\"\"\n",
    "    Calcule la similarité entre un segment DFA et un motif type en utilisant DTW.\n",
    "    \"\"\"\n",
    "    distance = dtw(segment, motif)\n",
    "    max_distance = max(np.linalg.norm(segment), np.linalg.norm(motif))\n",
    "    similarity = 1 - (distance / max_distance) if max_distance > 0 else 0\n",
    "    return similarity\n",
    "\n",
    "def recognize_activities(dfa_analysis, activity_motifs, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Identifie les activités sur le second jeu de données en comparant\n",
    "    chaque segment DFA aux motifs types avec un seuil de similarité.\n",
    "    \"\"\"\n",
    "    recognized_activities = []\n",
    "    \n",
    "    for day, results in dfa_analysis.dfa_results.items():\n",
    "        timestamps = np.array(results['timestamps'])\n",
    "        dfa_values = np.array(results['dfa_values'])\n",
    "        \n",
    "        for i, segment in enumerate(dfa_values):\n",
    "            best_match = None\n",
    "            best_score = 0\n",
    "            \n",
    "            for activity, motif in activity_motifs.items():\n",
    "                similarity = compute_similarity_score(segment, motif)\n",
    "                if similarity > best_score and similarity >= similarity_threshold:\n",
    "                    best_score = similarity\n",
    "                    best_match = activity\n",
    "            \n",
    "            if best_match:\n",
    "                recognized_activities.append({\n",
    "                    'timestamp': timestamps[i],\n",
    "                    'activity': best_match,\n",
    "                    'similarity': best_score,\n",
    "                    'date': timestamps[i].date()\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(recognized_activities)\n",
    "\n",
    "# Reconnaissance des activités\n",
    "recognized_activities_df = recognize_activities(dfa_analysis, activity_motifs, similarity_threshold=0.8)\n",
    "\n",
    "# Affichage des résultats avec courbe d'accélération, périodes d'inactivité et bandes colorées pour les activités\n",
    "def display_recognized_activities_with_acceleration(df, recognized_activities_df, inactivity_detector):\n",
    "    color_map = px.colors.qualitative.Set1\n",
    "    activity_types = recognized_activities_df['activity'].unique()\n",
    "    activity_colors = {activity: color_map[i % len(color_map)] for i, activity in enumerate(activity_types)}\n",
    "    \n",
    "    for day in recognized_activities_df['date'].unique():\n",
    "        daily_data = df[df['date_time'].dt.date == day]\n",
    "        daily_activities = recognized_activities_df[recognized_activities_df['date'] == day]\n",
    "        daily_inactivity = inactivity_detector.processed_data[inactivity_detector.processed_data['date'] == day]\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Courbe d'accélération\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=daily_data['date_time'], y=daily_data['acceleration'],\n",
    "            mode='lines', name='Acceleration', line=dict(color='blue', width=2)\n",
    "        ))\n",
    "        \n",
    "        # Ajout des périodes d'inactivité en transparence\n",
    "        inactive_added = False\n",
    "        for _, group in daily_inactivity.groupby('inactive_group'):\n",
    "            if group['inactive'].iloc[0]:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=group['epoch'], y=group['acceleration'],\n",
    "                    mode='lines', fill='tozeroy',\n",
    "                    fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "                    line=dict(width=0),\n",
    "                    name='Inactive Periods' if not inactive_added else None,\n",
    "                    showlegend=not inactive_added\n",
    "                ))\n",
    "                inactive_added = True\n",
    "        \n",
    "        # Ajout des activités reconnues sous forme de bandes colorées\n",
    "        for _, row in daily_activities.iterrows():\n",
    "            activity_color = activity_colors.get(row['activity'], 'rgba(0, 100, 255, 0.3)')\n",
    "            fig.add_shape(\n",
    "                type=\"rect\", x0=row['timestamp'], x1=row['timestamp'] + pd.Timedelta(seconds=10),\n",
    "                y0=daily_data['acceleration'].min(), y1=daily_data['acceleration'].max(),\n",
    "                fillcolor=activity_color, line=dict(width=0), opacity=0.3\n",
    "            )\n",
    "            fig.add_annotation(\n",
    "                x=row['timestamp'], y=daily_data['acceleration'].max(),\n",
    "                text=row['activity'], showarrow=False,\n",
    "                font=dict(size=10, color=\"black\"),\n",
    "                bgcolor=\"rgba(255,255,255,0.7)\"\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f\"Inactivity Periods, Acceleration & Recognized Activities - {day}\",\n",
    "            xaxis_title=\"Time\", yaxis_title=\"Acceleration\",\n",
    "            hovermode=\"x\", template=\"plotly_white\"\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "display_recognized_activities_with_acceleration(df1, recognized_activities_df, detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFAAnalysis:\n",
    "    def __init__(self, df, activity, inactivity_data, window_size=300, step_size=50):\n",
    "        \"\"\"\n",
    "        Initializes the DFAAnalysis class.\n",
    "\n",
    "        Parameters:\n",
    "        - df: DataFrame containing accelerometer time-series data.\n",
    "        - activity: DataFrame containing activity logs.\n",
    "        - inactivity_data: DataFrame indicating inactive periods.\n",
    "        - window_size: Number of points per DFA window.\n",
    "        - step_size: Step size for the sliding DFA window.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.activity = activity\n",
    "        self.inactivity_data = inactivity_data\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        self.dfa_results = {}\n",
    "        self.activity_colors = self.generate_activity_colors()\n",
    "\n",
    "    def generate_activity_colors(self):\n",
    "        \"\"\"Generates a unique color for each activity type.\"\"\"\n",
    "        unique_activities = self.activity['activite'].unique()\n",
    "        color_palette = plt.colormaps.get_cmap(\"tab10\")\n",
    "        return {activity: color_palette(i / len(unique_activities)) for i, activity in enumerate(unique_activities)}\n",
    "\n",
    "    def filter_active_periods(self):\n",
    "        \"\"\"Filters the DataFrame to exclude inactive periods.\"\"\"\n",
    "        \n",
    "        self.df['epoch'] = pd.to_datetime(self.df['epoch'])\n",
    "        self.inactivity_data['epoch'] = pd.to_datetime(self.inactivity_data['epoch'])\n",
    "\n",
    "        active_df = self.df.merge(self.inactivity_data[['epoch', 'inactive']], on='epoch', how='left')\n",
    "\n",
    "        if 'inactive_y' in active_df.columns:\n",
    "            active_df.rename(columns={'inactive_y': 'inactive'}, inplace=True)\n",
    "        if 'inactive_x' in active_df.columns:\n",
    "            active_df.drop(columns=['inactive_x'], inplace=True)\n",
    "        active_df['inactive'].fillna(False, inplace=True)\n",
    "        active_df = active_df[active_df['inactive'] == False].drop(columns=['inactive'])\n",
    "        return active_df\n",
    "    \n",
    "    def compute_dfa(self):\n",
    "        \"\"\"Computes the DFA exponent over active periods using a sliding window.\"\"\"\n",
    "        self.df['date'] = self.df['timestamp'].dt.date\n",
    "        unique_dates = self.df['date'].unique()\n",
    "\n",
    "        for day in unique_dates:\n",
    "            daily_df = self.df[self.df['date'] == day]\n",
    "            active_df = self.filter_active_periods()\n",
    "            active_daily_df = active_df[active_df['date'] == day]\n",
    "            series = active_daily_df['acceleration'].values\n",
    "            dfa_values = []\n",
    "            time_stamps = []\n",
    "\n",
    "            for i in range(0, len(series) - self.window_size, self.step_size):\n",
    "                window = series[i:i + self.window_size]\n",
    "                if len(window) == self.window_size:\n",
    "                    alpha = dfa(window)\n",
    "                    dfa_values.append(alpha)\n",
    "                    time_stamps.append(active_daily_df['timestamp'].iloc[i])\n",
    "\n",
    "            self.dfa_results[day] = {'timestamps': time_stamps, 'dfa_values': dfa_values}\n",
    "\n",
    "    def visualize_dfa(self):\n",
    "        \"\"\"Plots the DFA exponent over time for each day with activity periods highlighted and improved legend.\"\"\"\n",
    "        all_values = [val for res in self.dfa_results.values() for val in res['dfa_values']]\n",
    "        min_dfa, max_dfa = min(all_values), max(all_values)\n",
    "\n",
    "        for day, results in self.dfa_results.items():\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.plot(results['timestamps'], results['dfa_values'], marker='o', linestyle='-', label='DFA Exponent')\n",
    "            \n",
    "            legend_handles = []\n",
    "            activity_labels = set()\n",
    "            \n",
    "            for _, row in self.activity.iterrows():\n",
    "                if row['debut'].date() == day and row['activite'] not in activity_labels:\n",
    "                    legend_handles.append(mpatches.Patch(color=self.activity_colors[row['activite']], label=row['activite']))\n",
    "                    activity_labels.add(row['activite'])\n",
    "                    plt.axvspan(row['debut'], row['fin'], color=self.activity_colors[row['activite']], alpha=0.3)\n",
    "            \n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('DFA Exponent')\n",
    "            plt.ylim(min_dfa, max_dfa)\n",
    "            plt.title(f'DFA Exponent Over Time - {day}')\n",
    "            plt.legend(handles=legend_handles, title=\"Activities\", loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "            plt.show()\n",
    "\n",
    "    def correlate_dfa_with_activities(self):\n",
    "        \"\"\"Associates DFA values with recorded activities.\"\"\"\n",
    "        activity_dfa_mapping = {activity: [] for activity in self.activity['activite'].unique()}\n",
    "\n",
    "        for day, results in self.dfa_results.items():\n",
    "            for timestamp, alpha in zip(results['timestamps'], results['dfa_values']):\n",
    "                matching_activities = self.activity[(self.activity['debut'] <= timestamp) & (self.activity['fin'] >= timestamp)]\n",
    "                for _, row in matching_activities.iterrows():\n",
    "                    activity_dfa_mapping[row['activite']].append(alpha)\n",
    "\n",
    "        return activity_dfa_mapping\n",
    "\n",
    "    def plot_activity_dfa_distribution(self):\n",
    "        \"\"\"Plots the distribution of DFA exponent values for different activities.\"\"\"\n",
    "        activity_dfa_mapping = self.correlate_dfa_with_activities()\n",
    "        df_dfa = pd.DataFrame([(act, alpha) for act, alphas in activity_dfa_mapping.items() for alpha in alphas], \n",
    "                              columns=['Activity', 'DFA'])\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(data=df_dfa, x='Activity', y='DFA')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title('DFA Exponent Distribution by Activity')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#usage\n",
    "detector = InactivityDetector(df_resampled, activity)\n",
    "detector.detect_inactivity()\n",
    "inactivity_data = detector.processed_data\n",
    "detector.processed_data['timestamp'] = detector.processed_data['epoch']\n",
    "dfa_analysis = DFAAnalysis(detector.processed_data, activity, inactivity_data, window_size=300, step_size=50)\n",
    "dfa_analysis.compute_dfa()\n",
    "dfa_analysis.visualize_dfa()\n",
    "dfa_analysis.plot_activity_dfa_distribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
